{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "herbal-edinburgh",
   "metadata": {},
   "source": [
    "## Project Writeup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-listing",
   "metadata": {},
   "source": [
    "Author: Jinyue Zhu, Xianlin Shao"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-constraint",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-swift",
   "metadata": {},
   "source": [
    "In this project, we explore the climate and environment dataset (topic 2A) in the hope of exploring how real-world variables such as temperature, precipitation, wind speed varies with respect to seasons with a primary research goal of building a model to predict air pollution index (AQI) using time-series based data of human activities, greenhouse emission, and meteorology. \n",
    "\n",
    "Through walkthroughs on several important datasets published by GHCN and EPA, we aim to conduct data cleaning, EDA, and transformations to identify the relationship between the above variables. Then, several models including multiple linear regression, symbolic regression, multiple layer perceptron, and random forest regression are implemented using time-series-based data (created by the sliding window approach) to predict AQI in the Los Angeles region of California. These models are selected primarily because the current topic is a regression problem. For each model, we will work you through the process of hyperparameter tuning and the efforts we made on improving the accuracy. We will use the first 80% of 2016-2020's weekly summaries of the temperature, precipitation, CO emission, NO2 emission, SO2 emission, and wind condition for cross-validation and the training set to train the model, setting aside the last 20% for testing. In the following sections, we will walk you through the process of how we engineered the features from raw datasets and how we constructed the best-performing models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "several-custody",
   "metadata": {},
   "source": [
    "## Introduction and Literature Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-bouquet",
   "metadata": {},
   "source": [
    "Nowadays, climate change and air quality are hot topics for in global researcher community. As students equipped with knowledge in data science and machine learning, we hope to take advantage of the booming era of big data and contribute to the field by analyzing underlying trends in data and building a model to predict air quality.\n",
    "\n",
    "To better understand the proposed research questions, we have conducted literature reviews in the field, and the following studies have provided useful guidance. According to EPA, \"changes in climate can result in impacts to local air quality\" [1], as higher air temperatures can speed up chemical reactions in the air. The National Weather Service also points out that \"wind speed, air turbulence all affect how pollutants disperse or spread out from an area\" [2], thus influencing the air quality. On the other hand, \"rain typically results in less pollution since it washes away the particulate matter and can also wash out pollutants that are dissolvable\" [2]. Thus, we could see the potential influences of each variable we are examining on the air quality.\n",
    "\n",
    "Meanwhile, several past studies have tried to use machine learning to predict air quality. Yun-Chia et al. use a sliding window-based approach to process an 11-year dataset collected by Taiwan's Environmental Protection Administration, and then train the data with SVM, random forest, adaptive boosting, linear regression, and neural networks. With the best-tuned model and a sliding window of 1 hour, the models can all reach a $R^2$ result over 0.9, suggesting good fits. As the window gets wider, the fitness degrades [3]. On the other hand, using the air quality database from EPA to predict the air quality in California, Mauro also employs a time-series model, with sections of PCA and analysis for each kind of pollutants [4], which we find helpful. In addition to these research papers, we have also found guidance in several online blogs and tutorials [5-8], which cover best practices for handling time-series-based data and constructing models using sklearn.\n",
    "\n",
    "One novel aspect of our research as compared with contemporary research is the use of multi-field features to predict air quality. As compared with just using different kinds of pollutant emission for predicting air quality, we have carefully gathered temperature, precipitation, wind speed, and pollutant emission data in the city of LA over the years 2016-2020 to predict the region’s air quality. Our study’s exploration of the above features could provide insights to researchers in the field. Second, our research includes several aspects of hyperparameter tuning and feature engineering, such as the use of polynomial features and the inclusion of the previous week’s AQI as input features. Exploring these different feature sets allows us to conclude discoveries that are not yet in the current research community in the field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biological-detection",
   "metadata": {},
   "source": [
    "## Description of Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-microphone",
   "metadata": {},
   "source": [
    "### Breakdown of datasets (Individually)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-consequence",
   "metadata": {},
   "source": [
    "For the purpose of predicting California State's air quality index using various variables such as temperature, precipitation, wind speed, and pollutant concentration, we have gathered datasets from several credible sources. \n",
    "\n",
    "To gather weekly information on AQI, we use the Environmental Protection Agency (EPA)'s Air Data database. For temperature and precipitation data at the same level of granularity, we use National Centers for Environmental Information (NCEI)'s Global Historical Climatology Network Daily database. Meanwhile, for weekly wind speed, we use the NOAA database. Lastly, for weekly pollutants measurements such as CO, NO2, and SO2, we again refer to EPA's Air Data database. One of the benefits of using the above datasets is that the granularity match: they all contain daily measurements in the region we are interested in. Thus, we perform data preprocessing and cleaning to extract the desired information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-honor",
   "metadata": {},
   "source": [
    "#### Interesting findings on analyzing the AQI dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-sailing",
   "metadata": {},
   "source": [
    "The following three findings are based on several steps that we use to process the AQI dataset, with each row representing a daily measurement from a city in California in 2020. First, we inspect the dataset and found no outliers, but we found entries with NULL values in the CBSA_CODE column, which suggests that these measurements cannot be attributed to a given city. So, we dropped these rows of daily measurements. Then, we apply DateTime transformation to extract finer time details, such as the week, month, year, and day of the given measurement. For finding 1, we group by week, to extract the weekly average of AQI for a given region in CA. For finding 2, we group by week and month respectively on the processed dataset across all regions and then generate visualization based on the result. For finding 3, we extract the pm2.5 and AQI columns from the processed datasets and generate a scatter plot based on the two columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-standing",
   "metadata": {},
   "source": [
    "1. The AQI differs from one region to the other within the California state. As shown below in figure 1, we first grouped the AQI value by each CBSA (county-wise within CA), then we aggregate by month to get the monthly AQI. Each color line in the graph represents one location's monthly AQI curve over the year 2020. Here we use random sampling to avoid overplotting, but you can see that the monthly AQI follows a trend with respect to the season, but some locations' curves are shifted upward vertically, while some are shifted downward. Interestingly, the places with the lower curves are suburbs and non-industrialized cities, while the higher curves represent populated urban cities and cities with numerous factories. Additionally, the fact that most of the curves follow the same trend across the year suggests that the change in AQI also has to do with seasons. Note that during the transition from summer to fall, we see a peak in AQI, probably due to people start using ACs in the CA region. There are many more factors, the AC is just an example.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/geo_and_aqi.png\" style=\"width: 500px;\"/>\n",
    "    <span style=\"position:relative;top:-30px\">Figure 1. Relationship between geographical location and the AQI</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "remarkable-engineering",
   "metadata": {},
   "source": [
    "2. Another interesting discovery is about the pattern of AQI with respect to season, in a more granularized scenario. Now, we remove the geographical information, taking the average AQI across all regions in CA. Figure 2 below show the average AQI in CA each month and each week respectively. Now, if we pay more attention to the plot on the left, we see that the air quality gets better in spring, and then starts to decrease (higher AQI) in summer, peaking in September, and starts decreasing again. This aligns with our expectation, because we know one of the major sources that contribute to air quality is the number of greenhouse emissions. During the summer, especially in CA, the hot weather causes people to turn on their ACs, and that produce greenhouse gas (CO2). So, we see this peak. Then, in winter, people tend to use ACs less, and so the curve starts to decrease. This is one of the factors, but not all of the factors. So, we can conclude that August and September tend to be the months that have the highest AQI (highest level of air pollution in cities in CA). We can then use this piece of information in modeling. From the plot to the right, we now see a zig-zag-shaped oscillation of the curve that sorts of mimics the trend of the curve to the left if we \"smooth\" out the zig-zag part. This is again expected. By nature, we know many phenomenons oscillate with respect to time, and this is true for air quality as well. This gives us an idea of the level of granularity to be used for the data to be fed into our model, which predicts the air quality.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/month_week_and_aqi.png\" style=\"width: 700px;\"/>\n",
    "    <span style=\"position:relative;top:-20px\">Figure 2. Relationship of season and AQI</span>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-engineering",
   "metadata": {},
   "source": [
    "3. The last interesting finding is about the piecewise linear relationship between mean pm2.5 and the AQI. We all know that the AQI is determined based on a piecewise function that takes the mean pm2.5 measurements as input, but what exactly is this transformation function? Figure 3 gives you an idea. This is just for the curious reader, as people can directly find the transformation function online, but by using our sanitized data, we can also reconstruct this function with the use of scatterplot and overlaying line plots to distinguish one piecewise function from the other. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/pm_and_aqi.png\" style=\"width: 400px;\"/>\n",
    "    <span style=\"position:relative;top:-20px\">Figure 3. Piece-wise relationship between pm concentration and AQI</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-switch",
   "metadata": {},
   "source": [
    "#### Interesting findings on analyzing the Daily Global Weather dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-honduras",
   "metadata": {},
   "source": [
    "The following three findings are based on several steps that we use to process the Weather dataset, with each row representing a daily measurement from a city in California in 2020. First, we inspect the dataset and found no outliers, but the temperature and precipitation column has values that need to be divided by 10 to convert to the proper units, so we apply division on these two columns. Then, we apply DateTime transformation to extract finer time details, such as the week, month, year, and day of the given measurement. Next, we use a reverse geocoder to extract the state and country information for each row. Then, we group by week to extract the weekly summary for a given region in 2020. For finding 1, we extract several weekly summary temperature curves for different U.S. states, using random sampling of 50% from the state's list to avoid overplotting. For finding 2, we group by week and month respectively on the processed dataset across all regions, and then generative visualization based on the result. For finding 3, we extract the temperature and precipitation columns from the processed datasets and generate a scatter plot based on the two columns, with \"hue\" being the month column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "capital-funeral",
   "metadata": {},
   "source": [
    "1. Figure 4 displays the monthly temperature in 2020 over several U.S. states. We can see that there is indeed a trend between geographical location and the temperature variation over the seasons, this is evident because all lines follow a similar structure while being shifted vertically. This makes sense, because the closer the region or that state gets near the equator, the hotter it gets over the seasons, but all locations in the US transition in the same pattern across seasons (summer is the peak while it starts to go down in winter). For the curious readers, we extracted the geographical coordinates in the above plot. One of the high curves represents Florida. One of the lower curves represents Washington. Now, we cross-validate the trend with the monthly temperature data provided by NOAA and found that the results are consistent. This discovery is powerful because, from the previous section's analysis on AQI, we notice that AQI is influenced by season. The fact that temperature shares a similar trend means that the two variables could have some relationships, which we will explore in the next section.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/temp_and_state.png\" style=\"width: 500px;\"/>\n",
    "    <span style=\"position:relative;top:-20px\">Figure 4. Relationship between geographical location and the region's temperature</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-looking",
   "metadata": {},
   "source": [
    "2. Another interesting discovery is about the pattern of temperature with respect to season, in a more granularized scenario. Now, we remove the geographical information, taking the average temperature across all regions in the US. Figure 5 shows the average temperature in the US each month and each week respectively. From the plot to the left, we can confidently say that there is a correlation between a month and the average temperature for that month. The graph shows that, as we transition from spring to summer, the temperature gradually increases and stabilizes for a month during the hottest month (July & Aug), and then starts to decline as we enter the fall season. Note that the dataset ends in Oct 2020, so we cannot visualize the winter season here. However, based on the above graph, we can see there is a correlation between the month and the average temperature. Based on how season transitions, we can infer temperature when we are given what month it is. From the plot to the right, if we plot the weekly temperature, again we can see that we transition from a general trend to a more \"oscillation\" curve that still follows the monthly shape. This is expected because, in nature, the temperature fluctuates up and down, while the transition of seasons ensures the general trend. This gives us an idea of the level of granularity to be used for the data to be fed into the model.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/month_week_temp.png\" style=\"width: 700px;\"/>\n",
    "    <span style=\"position:relative;top:-20px\">Figure 5. Relationship of season and temperature</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-development",
   "metadata": {},
   "source": [
    "3. The last discovery of this dataset is about the relationship between temperature and precipitation across the months (averaged over all US weather stations). We have not mentioned too much about precipitation in this notebook, and that's because, from the EDA process, we found precipitation with respect to months and regions are less seasonal and hence did not indicate a clear relationship. However, from figure 6, we could kind of see that temperature and precipitation are correlated. Using the daily averaged temperature and precipitation from Jan 2020 to Oct 2020, we can see that as the temperature increases, the amount of precipitation tends to decrease. In Jan and Feb, when the temperature is cold, we tend to see larger precipitation more often. This trend fits neatly with the previously observed trend with respect to season, that in spring, we tend to see the most rainfall and near august, we tend to see the least (during July and August, we have the hottest days and we are less likely to see rains, and this is represented by the yellow and pink dots to the very right of the above graph). So, from this discovery, we can use temperature to infer the range of precipitation in the US region. If you bin the above scatter plot by temperature, we can see a different distribution of precipitation (in spring it is spread apart in the vertical axis, and in July and August it is quite clustered near 1 on the vertical axis). It also suggests that when having both temperature and precipitation as inputs to the model can be redundant, because precipitation has a relationship with temperature.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/temp_and_prcp.png\" style=\"width: 400px;\"/>\n",
    "    <span style=\"position:relative;top:-20px\">Figure 6. Relationship between temperature and precipitation</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-posting",
   "metadata": {},
   "source": [
    "#### Interesting findings on analyzing the pollutants dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-rating",
   "metadata": {},
   "source": [
    "The following two findings are based on several steps that we use to process the pollutants dataset, with each row representing a daily measurement of CO, NO2, SO2 from a city in California in 2020. First, we inspect the dataset and found no outliers. We then outer join the three tables together (one for CO, one for NO2, and one for SO2) by the measurement station. Then, we apply DateTime transformation to extract finer time details, such as the week, month, year, and day of the given measurement. Finally, we group by week to extract the weekly summary for a given region in 2020. For finding 1, we extract the max CO column from the processed dataset and plot it with respect to the week number. For finding 2, we extract the max CO and max NO2 columns and plot using a joint KDE plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-modification",
   "metadata": {},
   "source": [
    "1. Given that we would like to perform time-series-based prediction on AQI, we investigate the relationship of pollutants concentration with respect to time (in granularity of weeks). Figure 7 shows the weekly averaged CO emission measured in the California state. We see an interesting pattern of CO emission with respect to time, which correlates with previous visualizations on temperature with respect to time. We know that CO is mostly found in fumes produced by cars and power plants. Note how the emission curve peaks in August and September (summer season), when people in CA are turning on air conditioners to cool themselves. If you compare the temperature curve with the below one, we could see that human activities influence the emission A LOT. The air conditioner is just one example, and we can also list out the influence of traffic and etc. Note that, in the original notebook, we have also listed out the relationship of other pollutants (NO2, SO2) with respect to time. They share a similar pattern.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/week_and_co.png\" style=\"width: 400px;\"/>\n",
    "    <span style=\"position:relative;top:-0px\">Figure 7. Relationship between season and CO concentration</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-metadata",
   "metadata": {},
   "source": [
    "2. The other interesting discovery is about how the pollutants are correlated with one another. Figure 8 shows the joint distribution of daily CO and NO2 emissions in the California State in 2020. We see a positive correlation between the two pollutants using the joint KDE plot. This makes sense, as we know that cars, power plants produce a combination of CO and NO2. So, when we see the CO emission is high for a given day, we expect to see also high emission of NO2 in general. This discovery also suggests that maybe it is redundant to feed in both kinds of pollutants into the model to predict AQI, as they are correlated with one another largely.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/co_and_no2.png\" style=\"width: 400px;\"/>\n",
    "    <span style=\"position:relative;top:-0px\">Figure 8. Correlation between different kinds of pollutants</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-desktop",
   "metadata": {},
   "source": [
    "The above sections have included visualizations for interesting findings when processing the described datasets individually. For a detailed analysis of these datasets, please check out the individual analysis notebooks to see the complete data cleaning and EDA process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1237a15",
   "metadata": {},
   "source": [
    "#### Interesting findings on analyzing the wind dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3fc19d",
   "metadata": {},
   "source": [
    "The following findings are based on our investigation of the wind data set collected from NOAA ( National Oceanic and Atmospheric Administration). First, we inspect the dataset and found no outliers. Then we found that there are a lot of missing values in the dataset. Based on the investigation explained below, we decided to remove the stations with missing values. Then we filtered the data and only keep the records that are geologically close to Los Angeles. More specifically, we keep the measurements from the stations that are less than one degree longitude and latitude away from the center coordinate of Los Angelas. Finally, we take the weekly average of the wind speed and wind direction from these stations.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21599c5",
   "metadata": {},
   "source": [
    "1. The first thing we notice is the distribution of wind data records. Despite having a lot of missing wind speed (AWND) and wind direction(WDF2) values, the missing values are all from a few measuring stations that are close to some measurement stations with no missing values. As a result, it's safe to discard the measurements from stations with missing values. The distribution of the testing stations also suggests that lots of measurements have been conducted in and around Los Angelas Area. Due to this abundance of data, we decide to narrow down our prediction to Los Angelas Area.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/wind_1.png\" style=\"width: 600px;\"/>\n",
    "    <span style=\"position:relative;top:-0px\">Figure 9. Investigation on missing values in the wind dataset</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651422cc",
   "metadata": {},
   "source": [
    "2. Another interesting finding we got is that the wind speed and direction vary with season, as shown in Figure 10. We display a KDE plot of wind speed measurements and wind direction against the Datetime variable. We found that in Spring, Fall, and Winter, the wind speed measurement has a larger variance larger value. Wind speed in Summer is smaller than that of the other three seasons. We also see that in Summer, the wind direction is more concentrated at 260 degrees. Whereas in other seasons, the wind direction takes a larger variety of values.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/wind_2.png\" style=\"width: 600px;\"/>\n",
    "    <span style=\"position:relative;top:-0px\">Figure 10. Relationship of season with respect to wind speed and wind direction</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc29db8",
   "metadata": {},
   "source": [
    "3. The last interesting finding we got while investigating the wind dataset is that, after taking the weekly average on wind direction, we found that the weekly average wind direction comes from South and West, as shown in figure 11. Throughout the years in 2019 and 2020, none of the weekly average wind direction is from North and East. We think this is because Los Angeles is next to the Pacific Ocean and the wind in this area mainly blows from the ocean to the continent. In order to show this, we created the variables W, S, N, E to represent the wind direction. If the average wind direction comes from a certain direction, the feature of the corresponding direction will is 1. Otherwise, it takes value 0. As the graph shows, In most time, the average wind direction is West, and in other times, the wind comes from the South.\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/wind_3.png\" style=\"width: 400px;\"/>\n",
    "    <span style=\"position:relative;top:-0px\">Figure 11. Changes in wind direction with respect to time</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bacterial-billy",
   "metadata": {},
   "source": [
    "### Interesting Findinigs during Cross Datasets' EDA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-proof",
   "metadata": {},
   "source": [
    "We combine the above datasets through several steps. First, we use group by and aggregate to process each dataset to reach the same level of granularity (weekly summary). Then, we stack the columns together, using the time as the index. Essentially, we gather each kind of measurement for a given week from 2016-2020."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supported-mongolia",
   "metadata": {},
   "source": [
    "The below pairplots (figure 12) show a feature's relationship with respect to the weekly AQI, using combined datasets in the LA region over the years 2016 and 2020. From the previous section's analysis on individual datasets, we have concluded several useful discoveries, and we can see their effects within the below pairplot (the data plotted have been standardized).\n",
    "\n",
    "1. Temperature and weekly AQI is positively correlated. We get a sense of this when we see that the two variables share a similar pattern with respect to time. The pairplot validates the assumption. So, the weekly temperature can be used as a predictive feature, because as we see an increase in temperature, the AQI increases. \n",
    "\n",
    "2. Wind speed and weekly AQI is negatively correlated. This also makes sense, because with greater wind speed, the more capable that nature could blow the pm2.5 away, reducing the AQI. So, the weekly wind speed can also be used as a predictive feature.\n",
    "\n",
    "3. The \"W\" and \"S\" represent the wind direction. From the pairplot, we can see that the direction does not help too much in the regression setting, so we will remove these columns.\n",
    "\n",
    "4. Precipitation is not a strong predictor, i.e., we do not see a clear relationship between PRCP_MM and weekly AQI. We have congestion of samples near prcp=0, while the corresponding AQI is spread across the entire y-axis. This suggests that precipitation is not a good predictor for predicting AQI.\n",
    "\n",
    "5. Pollutant emission (CO, NO2, SO2) and weekly AQI is positively correlated. This also makes sense, because the more pollutants we emit, the higher the pollutant concentration and hence the larger the AQI value. However, the three plots (CO, NO2, SO2) share a similar trend. Further EDA suggests removing the redundant information, only using the NO2 and SO2 emission statistics for the models.\n",
    "\n",
    "6. Time_delta vs. weekly AQI resembles the AQI curve with respect to time. This makes sense because time_delta captures how far is the given week from the beginning week of the entire dataset, so it encodes the time duration. From the previous section, we know AQI is influenced by season, so the use of time_delta could help with the prediction.\n",
    "\n",
    "7. Week, the month also captures seasonal (or even more fine-grained time information), and hence could be helpful for the model. On the other hand, the year information does not help too much with the regression task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-depth",
   "metadata": {},
   "source": [
    "<img src=\"../figures/vars_to_aqi_0.png\" style=\"width: 600px;\"/>\n",
    "<img src=\"../figures/vars_to_aqi_1.png\" style=\"width: 600px;\"/>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/vars_to_aqi_2.png\" style=\"width: 600px;\"/>\n",
    "    <span style=\"position:relative;top:-0px\">Figure 12. Pairplot of each variable of interest with respect to AQI on weekly basis</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norman-detail",
   "metadata": {},
   "source": [
    "### Final Dataframe (merged from several datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-medicare",
   "metadata": {},
   "source": [
    "From the EDA process on the combined datasets, we have identified features that are not helpful with the regression task and hence removed them. Then, we merged the columns (after filtering out the redundant and non-useful features), applied standard scaling, and the final dataframe is shown below in table 1.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <span style=\"position:relative;top:10px\">Table 1. Final Dataframe merged from several sanitized datasets</span>\n",
    "    <img src=\"../figures/final_dataframe.png\" style=\"width: 700px;\"/>\n",
    "    <span style=\"position:relative;top:5px\"></span>\n",
    "</div>\n",
    "\n",
    "\n",
    "                                                   \n",
    "In this final dataframe, each row represents a given week's measurements for temperature, precipitation, CO, NO2, SO2, and AQI at the LA region for a given year. We now explain the meaning of each column one by one.\n",
    "- time_delta: a quantitative discrete variable that represents the difference in weeks of the given entry with respect to the first entry in the dataframe.\n",
    "- week: a quantitative discrete variable that represents the week number of that given entry in a given year.\n",
    "- year: a quantitative discrete variable that represents which year does that this weekly measurement is taken place.\n",
    "- month: a quantitative discrete variable that represents which month does this weekly measurement is taken place. \n",
    "- TEMP_C: a quantitative continuous variable that represents the weekly temperature in Celcius.\n",
    "- PRCP_MM: a quantitative continuous variable that represents the weekly precipitation in Millimeters.\n",
    "- weekly CO: a quantitative continuous variable that indicates the weekly averaged max CO measurement.\n",
    "- weekly NO2: a quantitative continuous variable that indicates the weekly averaged max NO2 measurement.\n",
    "- weekly SO2: a quantitative continuous variable that indicates the weekly averaged max SO2 measurement.\n",
    "- weekly AQI: a quantitative continuous variable that indicates the weekly averaged AQI measurement. This is our y column (prediction to be made)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-utility",
   "metadata": {},
   "source": [
    "## Description of Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "photographic-cooling",
   "metadata": {},
   "source": [
    "In this section, we discuss the usage of the sliding window approach (part of feature engineering), the selection of loss function, and the choice of regression models for the given task. In the next section, we will evaluate the performance of each model and discuss the model tuning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-container",
   "metadata": {},
   "source": [
    "#### Sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "partial-breakfast",
   "metadata": {},
   "source": [
    "We apply a sliding window to the final processed dataframe before sending it to the model. We make a reasonable assumption that a given week's AQI depends on the past several week's temperatures, precipitation, pollutants emission, and wind direction. This assumption is backed by the EDA discoveries shown in the previous sections. The choice of window size is a hyperparameter and can be tuned from model to model. Visually, a sliding window is explained below in figure 13, where we take the past few week's (t-3 to t) features as input to predict the given week's (t+1) AQI. We then repeat this process by sliding the window 1 step forward.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/sliding_window.png\" style=\"width: 400px;\"/>\n",
    "    <span style=\"position:relative;top:-0px\">Figure 13. Demonstration of the sliding window technique</span>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6cbf9",
   "metadata": {},
   "source": [
    "#### Train test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34522a6",
   "metadata": {},
   "source": [
    "Since we adopt the sliding window approach, that is, data of the first n days are used to predict the AQ value of the n+1 day, we can't use the regular approach of randomly shuffle the data and then split the training and testing datasets. This is because randomized splitting will cause part of testing data to be seen in the training set and it will break down the time sequencing relationship that we rely on to train the prediction model. Instead, we split the first 80% of data as a training set and leave the last 20% for testing. This splitting approach also reflects the primary research goal of this project, that is, to use the past data to predict future AQI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-damage",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-abuse",
   "metadata": {},
   "source": [
    "Please run the below cell to see the features we extracted from the training set, using a sliding window of 2. Note that we have applied a standard scaler to normalize the matrix, so the numbers may seem weird at first glance. For a given feature, the \"(x)\" at the end indicates this is the result from x-1 weeks prior. So, \"TEMP_C(1)\" represents the current week's temperature, \"TEMP_C(2)\" represents the last week's temperature, and etc. Each row represents a given sample to be feed into the model, as we are effectively saying that \"use the past two weeks (including the current week) features to predict the current week's AQI. For example, the first row represents using the 1st and 2nd week's features to predict the 2nd week's AQI, the second row represents using the 2nd week and 3rd week's features to predict the 3rd week's AQI. Depending on the window size, we have to drop certain starting and ending AQIs because of the sliding window approach.\n",
    "\n",
    "In terms of feature engineering, aside from extracting the past few week's temperatures, pollutant emission, wind speed, and etc. using the sliding window approach, we also engineered time-series based information such as the month number, week number, and the time_delta (current week's difference with respect to x-1 weeks earlier). This information has proven helpful because AQI varies with season. What would be a good feature to represent the season? It would be the month and week itself! Note that \"time_delta(1)\" is a column of all zeros. This is expected because the difference between the current week's time with the current week is always zero. We drop this column when feeding into the model. The reasons for selecting temperature, pollutant emission and wind speed as features have been explained in the \"Interesting Findings during Cross Datasets' EDA\", where we found a correlation of these variables with AQI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "federal-amazon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_delta(1)</th>\n",
       "      <th>time_delta(2)</th>\n",
       "      <th>week(1)</th>\n",
       "      <th>week(2)</th>\n",
       "      <th>year(1)</th>\n",
       "      <th>year(2)</th>\n",
       "      <th>month(1)</th>\n",
       "      <th>month(2)</th>\n",
       "      <th>TEMP_C(1)</th>\n",
       "      <th>TEMP_C(2)</th>\n",
       "      <th>...</th>\n",
       "      <th>weekly SO2(1)</th>\n",
       "      <th>weekly SO2(2)</th>\n",
       "      <th>weekly wind speed(1)</th>\n",
       "      <th>weekly wind speed(2)</th>\n",
       "      <th>W(1)</th>\n",
       "      <th>W(2)</th>\n",
       "      <th>S(1)</th>\n",
       "      <th>S(2)</th>\n",
       "      <th>∆t(3)</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.711445</td>\n",
       "      <td>-1.682189</td>\n",
       "      <td>-1.614461</td>\n",
       "      <td>-1.382428</td>\n",
       "      <td>-1.382428</td>\n",
       "      <td>-1.534785</td>\n",
       "      <td>-1.534785</td>\n",
       "      <td>-1.607167</td>\n",
       "      <td>-1.416260</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.778590</td>\n",
       "      <td>-0.400717</td>\n",
       "      <td>-0.042437</td>\n",
       "      <td>-1.089832</td>\n",
       "      <td>-1.565904</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>-0.632456</td>\n",
       "      <td>-3.409143</td>\n",
       "      <td>-6.230614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.697698</td>\n",
       "      <td>-1.614461</td>\n",
       "      <td>-1.546733</td>\n",
       "      <td>-1.382428</td>\n",
       "      <td>-1.382428</td>\n",
       "      <td>-1.534785</td>\n",
       "      <td>-1.534785</td>\n",
       "      <td>-1.416260</td>\n",
       "      <td>-0.772755</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.400717</td>\n",
       "      <td>-0.820001</td>\n",
       "      <td>-1.089832</td>\n",
       "      <td>-1.033011</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>-0.632456</td>\n",
       "      <td>-0.632456</td>\n",
       "      <td>-3.381650</td>\n",
       "      <td>-12.523886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.683952</td>\n",
       "      <td>-1.546733</td>\n",
       "      <td>-1.479004</td>\n",
       "      <td>-1.382428</td>\n",
       "      <td>-1.382428</td>\n",
       "      <td>-1.534785</td>\n",
       "      <td>-1.534785</td>\n",
       "      <td>-0.772755</td>\n",
       "      <td>-0.997982</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.820001</td>\n",
       "      <td>0.528066</td>\n",
       "      <td>-1.033011</td>\n",
       "      <td>0.572751</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>-0.632456</td>\n",
       "      <td>-0.632456</td>\n",
       "      <td>-3.354157</td>\n",
       "      <td>-10.687204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.670205</td>\n",
       "      <td>-1.479004</td>\n",
       "      <td>-1.411276</td>\n",
       "      <td>-1.382428</td>\n",
       "      <td>-1.382428</td>\n",
       "      <td>-1.534785</td>\n",
       "      <td>-1.238413</td>\n",
       "      <td>-0.997982</td>\n",
       "      <td>-1.233934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.528066</td>\n",
       "      <td>-0.343777</td>\n",
       "      <td>0.572751</td>\n",
       "      <td>0.907062</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>-1.565904</td>\n",
       "      <td>-0.632456</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>-3.326663</td>\n",
       "      <td>19.337982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.656458</td>\n",
       "      <td>-1.411276</td>\n",
       "      <td>-1.343548</td>\n",
       "      <td>-1.382428</td>\n",
       "      <td>-1.382428</td>\n",
       "      <td>-1.238413</td>\n",
       "      <td>-1.238413</td>\n",
       "      <td>-1.233934</td>\n",
       "      <td>0.136732</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.343777</td>\n",
       "      <td>2.430740</td>\n",
       "      <td>0.907062</td>\n",
       "      <td>-0.759126</td>\n",
       "      <td>-1.565904</td>\n",
       "      <td>-1.565904</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>-3.299170</td>\n",
       "      <td>-6.134097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.955385</td>\n",
       "      <td>0.823759</td>\n",
       "      <td>0.891488</td>\n",
       "      <td>0.776902</td>\n",
       "      <td>0.776902</td>\n",
       "      <td>0.836193</td>\n",
       "      <td>0.836193</td>\n",
       "      <td>1.275737</td>\n",
       "      <td>1.112715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007757</td>\n",
       "      <td>-0.309322</td>\n",
       "      <td>-0.169450</td>\n",
       "      <td>0.753108</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>-1.565904</td>\n",
       "      <td>-0.632456</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>1.924516</td>\n",
       "      <td>5.929686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.969131</td>\n",
       "      <td>0.891488</td>\n",
       "      <td>0.959216</td>\n",
       "      <td>0.776902</td>\n",
       "      <td>0.776902</td>\n",
       "      <td>0.836193</td>\n",
       "      <td>0.836193</td>\n",
       "      <td>1.112715</td>\n",
       "      <td>0.387700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.309322</td>\n",
       "      <td>1.025068</td>\n",
       "      <td>0.753108</td>\n",
       "      <td>-0.712448</td>\n",
       "      <td>-1.565904</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>-0.632456</td>\n",
       "      <td>1.952009</td>\n",
       "      <td>5.123000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.982878</td>\n",
       "      <td>0.959216</td>\n",
       "      <td>1.026944</td>\n",
       "      <td>0.776902</td>\n",
       "      <td>0.776902</td>\n",
       "      <td>0.836193</td>\n",
       "      <td>1.132566</td>\n",
       "      <td>0.387700</td>\n",
       "      <td>0.795253</td>\n",
       "      <td>...</td>\n",
       "      <td>1.025068</td>\n",
       "      <td>0.698367</td>\n",
       "      <td>-0.712448</td>\n",
       "      <td>0.089851</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>-1.565904</td>\n",
       "      <td>-0.632456</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>1.979502</td>\n",
       "      <td>-2.195346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.996624</td>\n",
       "      <td>1.026944</td>\n",
       "      <td>1.094673</td>\n",
       "      <td>0.776902</td>\n",
       "      <td>0.776902</td>\n",
       "      <td>1.132566</td>\n",
       "      <td>1.132566</td>\n",
       "      <td>0.795253</td>\n",
       "      <td>0.576461</td>\n",
       "      <td>...</td>\n",
       "      <td>0.698367</td>\n",
       "      <td>0.802633</td>\n",
       "      <td>0.089851</td>\n",
       "      <td>-0.084482</td>\n",
       "      <td>-1.565904</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>-0.632456</td>\n",
       "      <td>2.006995</td>\n",
       "      <td>-9.420394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.010371</td>\n",
       "      <td>1.094673</td>\n",
       "      <td>1.162401</td>\n",
       "      <td>0.776902</td>\n",
       "      <td>0.776902</td>\n",
       "      <td>1.132566</td>\n",
       "      <td>1.132566</td>\n",
       "      <td>0.576461</td>\n",
       "      <td>1.887067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802633</td>\n",
       "      <td>1.308299</td>\n",
       "      <td>-0.084482</td>\n",
       "      <td>-0.049003</td>\n",
       "      <td>0.638609</td>\n",
       "      <td>-1.565904</td>\n",
       "      <td>-0.632456</td>\n",
       "      <td>1.581139</td>\n",
       "      <td>2.034488</td>\n",
       "      <td>10.678519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     time_delta(1)  time_delta(2)   week(1)   week(2)   year(1)   year(2)  \\\n",
       "0              0.0      -1.711445 -1.682189 -1.614461 -1.382428 -1.382428   \n",
       "1              0.0      -1.697698 -1.614461 -1.546733 -1.382428 -1.382428   \n",
       "2              0.0      -1.683952 -1.546733 -1.479004 -1.382428 -1.382428   \n",
       "3              0.0      -1.670205 -1.479004 -1.411276 -1.382428 -1.382428   \n",
       "4              0.0      -1.656458 -1.411276 -1.343548 -1.382428 -1.382428   \n",
       "..             ...            ...       ...       ...       ...       ...   \n",
       "194            0.0       0.955385  0.823759  0.891488  0.776902  0.776902   \n",
       "195            0.0       0.969131  0.891488  0.959216  0.776902  0.776902   \n",
       "196            0.0       0.982878  0.959216  1.026944  0.776902  0.776902   \n",
       "197            0.0       0.996624  1.026944  1.094673  0.776902  0.776902   \n",
       "198            0.0       1.010371  1.094673  1.162401  0.776902  0.776902   \n",
       "\n",
       "     month(1)  month(2)  TEMP_C(1)  TEMP_C(2)  ...  weekly SO2(1)  \\\n",
       "0   -1.534785 -1.534785  -1.607167  -1.416260  ...      -0.778590   \n",
       "1   -1.534785 -1.534785  -1.416260  -0.772755  ...      -0.400717   \n",
       "2   -1.534785 -1.534785  -0.772755  -0.997982  ...      -0.820001   \n",
       "3   -1.534785 -1.238413  -0.997982  -1.233934  ...       0.528066   \n",
       "4   -1.238413 -1.238413  -1.233934   0.136732  ...      -0.343777   \n",
       "..        ...       ...        ...        ...  ...            ...   \n",
       "194  0.836193  0.836193   1.275737   1.112715  ...       0.007757   \n",
       "195  0.836193  0.836193   1.112715   0.387700  ...      -0.309322   \n",
       "196  0.836193  1.132566   0.387700   0.795253  ...       1.025068   \n",
       "197  1.132566  1.132566   0.795253   0.576461  ...       0.698367   \n",
       "198  1.132566  1.132566   0.576461   1.887067  ...       0.802633   \n",
       "\n",
       "     weekly SO2(2)  weekly wind speed(1)  weekly wind speed(2)      W(1)  \\\n",
       "0        -0.400717             -0.042437             -1.089832 -1.565904   \n",
       "1        -0.820001             -1.089832             -1.033011  0.638609   \n",
       "2         0.528066             -1.033011              0.572751  0.638609   \n",
       "3        -0.343777              0.572751              0.907062  0.638609   \n",
       "4         2.430740              0.907062             -0.759126 -1.565904   \n",
       "..             ...                   ...                   ...       ...   \n",
       "194      -0.309322             -0.169450              0.753108  0.638609   \n",
       "195       1.025068              0.753108             -0.712448 -1.565904   \n",
       "196       0.698367             -0.712448              0.089851  0.638609   \n",
       "197       0.802633              0.089851             -0.084482 -1.565904   \n",
       "198       1.308299             -0.084482             -0.049003  0.638609   \n",
       "\n",
       "         W(2)      S(1)      S(2)     ∆t(3)          Y  \n",
       "0    0.638609  1.581139 -0.632456 -3.409143  -6.230614  \n",
       "1    0.638609 -0.632456 -0.632456 -3.381650 -12.523886  \n",
       "2    0.638609 -0.632456 -0.632456 -3.354157 -10.687204  \n",
       "3   -1.565904 -0.632456  1.581139 -3.326663  19.337982  \n",
       "4   -1.565904  1.581139  1.581139 -3.299170  -6.134097  \n",
       "..        ...       ...       ...       ...        ...  \n",
       "194 -1.565904 -0.632456  1.581139  1.924516   5.929686  \n",
       "195  0.638609  1.581139 -0.632456  1.952009   5.123000  \n",
       "196 -1.565904 -0.632456  1.581139  1.979502  -2.195346  \n",
       "197  0.638609  1.581139 -0.632456  2.006995  -9.420394  \n",
       "198 -1.565904 -0.632456  1.581139  2.034488  10.678519  \n",
       "\n",
       "[199 rows x 24 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"../data/train.pkl\", \"rb\") as f:\n",
    "    train_window = pickle.load(f)\n",
    "    \n",
    "train_window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-second",
   "metadata": {},
   "source": [
    "#### Loss Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "essential-median",
   "metadata": {},
   "source": [
    "Because we are handling a regression problem, we use root mean squared loss (RMSE) to evaluate a given model's performance. RMSE is used both in training models and in evaluating model performance. We did not consider mean absolute error because we want to penalize errors with greater force."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "successful-capital",
   "metadata": {},
   "source": [
    "#### Modeling Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb825319",
   "metadata": {},
   "source": [
    "In terms of the modeling process, we first process each individual dataset on temperature, precipitation, wind speed, AQI, and pollutants, constraining the scope of the data to be at the LA region, removing outliers, cleaning the data. Then, we aggregate the data by taking the weekly summaries and then merge the datasets together, so that each row in the final data frame contains the summary statistic of temperature, precipitation, wind speed, etc. for a given week between year 2016 and 2020. There are several pipeline functions defined in the modeling notebook that serve the exact purposes described above. With the processed dataset, we then apply a sliding window to engineer features that we plan to use for predicting AQI, i.e., we slide the window across the dataset, creating each row that represents the past few week’s temperatures, precipitation, etc. and designates the y variable to be the current week’s AQI. We believe this approach is valid because of our findings in the EDA section, in which we showed that AQI has a relationship with time and features such as the past few week’s wind speed and pollutant emissions. Additionally, to better fit the seasonal trend in the AQI curve, we engineered polynomial features on the processed data and considered including the previous week’s AQI as input features. These two additional ways could improve the accuracy of the model, and the reasons are discussed in the evaluation section. Then, the processed data gets split into train and test sets. Because we are dealing with time-based data, we set the split to be the start of 2020. So, we will use 2016-2019’s data for training and cross-validation, and we will only use 2020’s data as the test set, to evaluate the model’s performance on unseen data. Next, 4 models are proposed, which are linear regression, symbolic regression, multiple layer perceptron, and random forest regression. The reasons for choosing these models are illustrated in detail in the following sections. For linear regression (ridge) and multiple-layer perceptron, we use the alpha term in hyperparameter tuning to control the degree of overfitting. For random forest regression, we use the tree depth and the number of trees in the forest to control overfitting. We do not handle overfitting for the case of symbolic regression, as the topic is beyond the scope of this course. However, we can tune the function set for symbolic regression and the number of generations to evolve. The modeling notebook contains a cross-validation function written for each model to handle the hyperparameter tuning process. Metric-wise, we use the root mean squared error (rmse) to evaluate the model’s performance. We also include nice visualization on each best-tuned model’s result on train and test set. \n",
    "\n",
    "__Evaluation of your approach and discuss any limitations of the methods__\n",
    "We believe our approach of using time-series-based temperature, precipitation, pollutant emission, and wind speed to predict air quality is legit. Within our capabilities, we have gathered datasets that contain the above information. However, through literature review, there are other features such as air turbulence, traffic activity, special activity (wildfire and fireworks) and etc. that could also influence the AQI in a significant way. Unfortunately, we cannot find a relevant dataset that has the desired granularity. A yearly traffic report is way too generalized for our application. So, if we were able to gather the above features, then we believe we could further improve our model’s fit.\n",
    "\n",
    "We think the use of a sliding window is necessary to predict AQI values that vary with time. However, as we explore through the feature engineering and model tuning process, we found that we need more powerful features or the ability to model non-linear cyclic curves that occur in the AQI curve. So, in retrospect, we believe it is important to select models that could model the cyclic pattern, such as the neural network (MLP). The use of polynomial features is another way to fix the issue, but this approach could potentially introduce redundant features and drag down the training speed for models like linear regression and symbolic regression. We also think there should be a better way to model the periodic pattern in the AQI curve. In the future, we could try using a carefully designed sine/cosine lifting map on our feature.\n",
    "\n",
    "Our approach has the following limitations. First, we are conducting this research topic on a weekly scale. We think this is a good tradeoff between generalizing too much (monthly estimates) and fitting to noises (daily estimate). However, there should be a way to better tune this parameter, but in the interest of time and the ease to process using pandas, we selected weekly estimates. Second, the multiple-layer perceptron is bad for inference. Though it provides a good fit, as humans it is hard to know what new features or patterns the neural network has learned. And it is also hard to tune a multiple-layer perceptron’s hidden layer exhaustively. We don’t have that computational power to do so. Lastly, we believe the wildfire in the CA state in 2020 could be one of the reasons for the unusually sharp curves in the summertime. That creates difficulty for the model to predict in that scenario. If we could find datasets regarding these special events in timestamps and take this information into the model, our fit could be better potentially.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c00f45",
   "metadata": {},
   "source": [
    "#### Cross-Validation on hyperparameters and regularization parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe369474",
   "metadata": {},
   "source": [
    "For every model, we define a corresponding cross-validation function to tune its model-specific hyperparameters as well as regularization parameters. \n",
    "\n",
    "For linear regression with l2 regularization penalty, we tune the alpha parameter. For symbolic regression, we tune for the number of generations and population size. For multiple layer perceptron, we tune for the alpha (l2 regularization penalty), and the hidden layer dimensions. For random forest regression, we tune for the number of trees, the proportion of samples to be used in each tree, and the proportion of features to be used in each split.\n",
    "\n",
    "For each model, the k-fold cross validation function takes in several lists, each of which contains the candidate value of the hyperparameters to be tuned. \n",
    "\n",
    "The hyperparameters are tuned on the training set. The training set is split into k folds. k-1 folds which are used in training the model on the specific combination of parameters and the remaining 1 fold are used for validation. We repeat this process k times for each set of parameters. Then, we take the average as the final validation score. Because we are handling a regression problem, we use rose as the evaluation metric. The lower the score, the better fit the model achieves. Finally, we use the best-tuned parameters to construct the final model. The final model is trained on the entire training set and evaluates on the test set. Note that, during cross-validation, the model never gets to learn from the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bound-heading",
   "metadata": {},
   "source": [
    "#### Model 1: Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-alliance",
   "metadata": {},
   "source": [
    "In this case, we model AQI ($\\hat{y}$) as resulting from a linear combination of features multiplied by weights and one additional bias term. Formally: \n",
    "\n",
    "$$\\hat{y} = \\sum_{i=0}^{k} \\beta_{i}x_{i} + \\alpha$$\n",
    "\n",
    "where x is the feature vector, beta is the weight vector and alpha is the bias term. With the sliding window processed features, we are using this model to see if we can find a good fit for a given week's AQI using a linear combination of features for the past few weeks. From the EDA section, we've seen that some features (temperature, wind speed, and etc) are correlated with AQI in a near-linear way. The use of linear regression helps us check the hypothesis on whether the AQI can be predicted using a linear combination of the features listed above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-parish",
   "metadata": {},
   "source": [
    "#### Model 2: Symbolic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "according-encyclopedia",
   "metadata": {},
   "source": [
    "In this case, we model AQI ($\\hat{y}$) as the result of a combination of mathematical expressions operated on the sliding window's processed features. This model is more powerful than model 1 because it allows more complex relationships to be expressed on input features, such as sine waves, exponentials, multiplication, and etc. For example, the symbolic regression is capable of learning the below function during training, as shown in figure 14.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/sym_reg.png\" style=\"width: 300px;\"/>\n",
    "    <span style=\"position:relative;top:-0px\">Figure 14. An example function that symbolic regression is capable of learning</span>\n",
    "</div>\n",
    "\n",
    "With the sliding window's processed features, we are using this model to see if we can find a good fit for a given week's AQI using a set of mathematical functions operating on the features for the past few weeks. From the EDA section, we've also seen features (week, month, and etc) that have a non-linear relationship with AQI. In particular, the time-series information indicates some level of oscillation over the months, suggesting a relationship of AQI with respect to seasons. So, we hope the non-linear (multiply, sine, and etc) functions provided by symbolic regression could model this non-linear periodic pattern and achieve a better fit than linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "balanced-variance",
   "metadata": {},
   "source": [
    "#### Model 3: Multiple Polynomial Regression (Model 1 with polynomial features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-syndrome",
   "metadata": {},
   "source": [
    "In this case, we model AQI ($\\hat{y}$) as resulting from a linear combination of features (up to polynomial degree p) multiplied by weights and one additional bias term. Formally:\n",
    "\n",
    "$$\\hat{y} = \\sum_{i=0}^{m} \\beta_{i}Poly(x_{i}) + \\alpha$$\n",
    "\n",
    "where x is the feature vector, beta is the weight vector and alpha is the bias term. The Poly() function takes the input features and lifts them to a resulting vector of polynomial degree p. For example, if $x = [x_1, x_2]$, then $Poly(x, p=2) = [x_1^{2}, x_2^{2}, x_1x_2, x_1, x_2]$. The selection of this model follows the same reasoning as that of model 2. Given that we found some periodicity non-linear pattern associated with AQI, we want to investigate the use of the higher dimensional feature (square terms for month, time_delta) to the linear model and see if model 3 could do better than model 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "insured-synthesis",
   "metadata": {},
   "source": [
    "#### Model 4: Multiple Layer Perceptron (MLP) Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-philippines",
   "metadata": {},
   "source": [
    "In this case, we model AQI ($\\hat{y}$) as the output value computed through a neural network, which takes in the input features in the very first layer. The number of layers and the number of nodes on each layer are hyperparameters to be tuned through cross-validation. The computational intensive layers of nodes allow neural networks to learn relationships and even new features that are beyond human interpretation. In fact, the universal approximation property suggests that an MLP can solve any regression problem at the desired accuracy if it has enough hidden neurons and training data to learn parameters for all the hidden layers. From the CS289 course, we also learned that MLP also benefits from model structure and regularization, which reduces overfitting. We include this model as we are curious about its performance as compared with other traditional regression models. We also believe this model should achieve the highest fit among all models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-workstation",
   "metadata": {},
   "source": [
    "#### Model 5: Random Forrest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "right-switch",
   "metadata": {},
   "source": [
    "In this case, we model AQI ($\\hat{y}$) as the output value aggregated by a set of trees, each trained on a different subsample of the training dataset and also a subsample of features. We want to try this model because we want to explore the use of ensemble learning on regression tasks and compare the result with other non-ensemble methods. With the use of bagging and a random sample of features at each split for each tree in the forest, we effectively generate more randomness and allow the forest to grow differently. If the trees are very similar, then taking their average doesn't reduce the variance much. Meanwhile, we tune each tree's depth to control the bias (deeper trees have a lower bias). Hence, random forests enable us to listen to the collective wisdom of a group of \"weak\" learners while preserving the ability to tune for the best spot in the bias-variance trade-off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-franchise",
   "metadata": {},
   "source": [
    "## Discussion of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-sacrifice",
   "metadata": {},
   "source": [
    "#### Model 1: Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eligible-vessel",
   "metadata": {},
   "source": [
    "__Modeling without Ploynomial features__\n",
    "We trained two Multiple Linear Regression models. The first one is trained on linear features and the second one includes the polynomial lifted features. The results produced below are the best-tuned models after hyperparameter tuning.\n",
    "\n",
    "The best Multiple Linear Regression without polynomial feature with tuned regularization parameter generated a training rmse of 8.15 and testing of 12.99. Figure 15 demonstrates the performance visually, plotting the predicted y and the actually on both the training set and test set.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/Multiple_Linear_Regression_wo_ploy_1.png\" style=\"width: 600px;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/Multiple_Linear_Regression_wo_ploy_2.png\" style=\"width: 600px;\"/>\n",
    "    <span style=\"position:relative;top:-0px\">Figure 15. Plots of predicted y and actual y evaluated on the training set and test set, using linear model without polynomial features</span>\n",
    "</div>\n",
    "\n",
    "As we see, the model was able to capture the AQI increase and decrease trend. But it's not able to predict correctly the magnitude of AQI fluctuation. Our best-tuned model has an l2 regularization parameter of 45. As we observe from the graph, the prediction value has high fluctuation, and thus predicting the value with the wrong sign would highly impact rmse rate. As a result, a large regularization penalty would help avoid this problem.\n",
    "\n",
    "__Modeling with Polynomial features__\n",
    "The Multiple Linear Regression with polynomial feature gives us a slightly better prediction. It generates a training rmse of 9.06 and a testing rmse of 12.29. With polynomial features, we need to penalize more, using a tuned alpha=65. However, now it cannot capture the spikes in the test set. This is because of the use of rmse as a loss function and the high l2 penalty, it is penalizing large deviations from the correct value, as suggested by the curve on the training set. The large spikes in the test set make it hence difficult to predict by the best-tuned model. The following figure 16 demonstrates the result visually.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/Multiple_Linear_Regression_with_ploy_1.png\" style=\"width: 600px;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/Multiple_Linear_Regression_with_ploy_2.png\" style=\"width: 600px;\"/>\n",
    "    <span style=\"position:relative;top:-0px\">Figure 16. Plots of predicted y and actual y evaluated on the training set and test set, using linear model with polynomial features</span>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liked-auction",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescribed-granny",
   "metadata": {},
   "source": [
    "__Bias and variance tradeoff__\n",
    "Our model has a high l2 panulty term and is heavily biased. As the graph demonstrate, our prediction data has high fluctuation. Therefore the data potentially has high noise. In this case, high reglarization allows us to capture the major trend of data without sacrifying too mch due to noises."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moving-matrix",
   "metadata": {},
   "source": [
    "__Limitation:__ The linear model after parameter tuning still cannot properly capture the shape of the AQI curve, as suggested by the plot for the training set's y and y_pred. This suggests that we should either engineer more useful features or try a more powerful model that can bring in non-linearity to better fit the curve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-plain",
   "metadata": {},
   "source": [
    "#### Model 2: Symbolic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-humanity",
   "metadata": {},
   "source": [
    "The Symbolic regression in some sense is like the Multiple Layer Perceptron, but in a tree-like structure with less computational power. The ability to use a function set including non-linear functions on features such as multiplication and sine could help with modeling seasonal trends. However, the symbolic regression is hard to tune, timely to tune and the process of how the tree is built is beyond the scope of this course. Moreover, by our observation, to reach a relatively good fit, the tree structure often reaches 200 in-depth, which could suggest overfitting and it is inefficient to train. Due to the insane amount of time, it takes to train and the ability of the symbolic regressor to generate polynomial features, our cross-validation routine for symbolic regression only considers normal features. Result-wise, it produces a curve similar to the linear regression case, as shown in figure 17. We can also see that it does decently well on the test set, in terms of predicting the location of the spike and the magnitude.\n",
    "\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/sym_1.png\" style=\"width: 600px;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/sym_2.png\" style=\"width: 600px;\"/>\n",
    "    <span style=\"position:relative;top:-0px\">Figure 17. Plots of predicted y and actual y evaluated on the training set and test set, uisng symbolic regression</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "double-sigma",
   "metadata": {},
   "source": [
    "##### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-judge",
   "metadata": {},
   "source": [
    "__Bias and variance tradeoff:__ This model has a very low bias due to its ability to fit highly flexible basis functions. However, due to the randomness inherent in the genetic component of the algorithm, the model has a high variance–generating different features on each run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gross-demographic",
   "metadata": {},
   "source": [
    "__Limitation:__ Symbolic regression, though flexible, is prone to overfitting when the atom functions are many. Furthermore, even when properly fit, the features are not easily explained. For instance $cos(sin(x_1^2) + log(x_1x_2)^3)$ is a valid feature, but totally inscrutable. TODO: INSERT COMPLICATED COEF FROM RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "velvet-allocation",
   "metadata": {},
   "source": [
    "#### Model 3: Multiple Layer Perceptron (MLP) Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-stewart",
   "metadata": {},
   "source": [
    "The MLP performs the best as compared to the other three models. As a neural network, it is capable of mapping a layer of inputs to the next layer in a fully connected manner, hence the knowledge learned by the regressor is maximized. Additionally, thanks to the efficient backdrop algorithm, the MLP is fast to train, even though the computation is large, the backprop can carry the partial computation to the earlier layer and hence avoid unnecessary work. As indicated by figure 18, the MLP is sufficient enough to capture the trend in AQI and hence reach great performance on the test set. We can see that the curve predicted by MLP on the test set follows the same seasonal pattern and every spike and trough is aligned, though the magnitude can differ a bit. For this best-tuned model, we are using a sliding window of size 2, an l2 penalization of 45, a 7 layer network with five hidden layers of size (2048, 1024, 512, 256, 32).\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/mlp_with_ploy_1.png\" style=\"width: 600px;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/mlp_with_ploy_2.png\" style=\"width: 600px;\"/>\n",
    "    <span style=\"position:relative;top:-0px\">Figure 18. Plots of predicted y and actual y evaluated on the training set and test set, using MLP</span>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cross-class",
   "metadata": {},
   "source": [
    "##### Evaluation\n",
    "\n",
    "The MLP regressor demonstrated great performance and the best fit to the up and downs in the test set. The training prediction plot shows that the model has enough capacity to capture the noise present in the dataset. And on the test set, we also see better prediction results as compared to other models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dying-wealth",
   "metadata": {},
   "source": [
    "__Bias and variance tradeoff:__ MLP regression is another low-bias model, as NNs are intended as universal function approximators. We use cross-validation to tune for the best spot without getting an exploding variance. The regularization used in this model increases the bias slightly (tending towards a constant predictor) but keeps the model from severely overfitting so that we are able to get the best performance on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-ceiling",
   "metadata": {},
   "source": [
    "__Limitation:__ As wth symbolic regression, MLP regression is not interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da4413",
   "metadata": {},
   "source": [
    "#### Model 4: Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-lodge",
   "metadata": {},
   "source": [
    "The following two graphs demonstrate the best tuned random forest model without and with polynomial features. We can see that the performance does not change much. If the individual decision trees failed to capture the intrinsic trend in the AQI curve, then the ensemble of those trees for regression will create a lot of noise, and though the averaging of these values tend to be stable, they failed to capture the periodic sharp transitions in seasons and hence failed to predict the trend in the test set. Hence, we don't consider ensemble learning being super helpful in predicting the trends in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e9ebee",
   "metadata": {},
   "source": [
    "__Modeling without Ploynomial features__\n",
    "\n",
    "For this best-tuned model, we are using a sliding window of size 2 and 900 trees in the forest. The performance is visualized in figure 19.\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/rf_wo_ploy_1.png\" style=\"width: 600px;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/rf_wo_ploy_2.png\" style=\"width: 600px;\"/>\n",
    "    <span style=\"position:relative;top:-0px\">Figure 19. Plots of predicted y and actual y evaluated on the training set and test set, using random forest regression without polynomial features</span>\n",
    "</div>\n",
    "\n",
    "\n",
    "__Modeling with Polynomial features__\n",
    "\n",
    "For this best-tuned model, we are using a sliding window of size 2, 300 trees in the forest, and polynomial features of degree 3. The performance is visualized in figure 20. Note that with the introduction of polynomial features, the model could predict the turning points (transition from up and down) on the test set accurately. However, the magnitude of the spikes cannot be predicted appropriately. This is a result of ensemble learning taking the average across all trees (averaging effect) and the use of regularization.\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/rf_with_ploy_1.png\" style=\"width: 600px;\"/>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "    <img src=\"../figures/rf_with_ploy_2.png\" style=\"width: 600px;\"/>\n",
    "    <span style=\"position:relative;top:-0px\">Figure 20. Plots of predicted y and actual y evaluated on the training set and test set, using random forest regression with polynomial features</span>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ddc0c",
   "metadata": {},
   "source": [
    "##### Evaluation\n",
    "\n",
    "The random forest model was able to achieve a small rmse, but it could not capture the spikes in the test set. Despite massively overfitting the training set, the model's feature selection was robust to the distribution of the test set, so at least it knows where the spikes would occur. However, as compared to other models, this model cannot predict the magnitude of the spikes in the test set properly, and maybe this has to do with the averaging of all individual learner's results and the fact that some important features are not selected during splits. Hence, it is not preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bebb19b",
   "metadata": {},
   "source": [
    "__bias and variance tradeoff:__ The RF model's bias-variance tradeoff depends on the depth of the tree and the capacity of the weak learner. The model used in these experiments uses 100 estimators, which is a decent number. Overfitting is controlled through bagging (averaging), which encourages a low-bias model.\n",
    "\n",
    "__Limitations:__ Random forests require a lot of hyperparameter tuning. It is hard to cross-validation everything exhaustively, and so we may not have found the best spot for this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "promising-engagement",
   "metadata": {},
   "source": [
    "### Summary of all models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-kentucky",
   "metadata": {},
   "source": [
    "So, performance wise, we conclude that:\n",
    "\n",
    "$$Random Forest Regression < Multiple Linear Regression <= Symbolic Regression < Multiple Layer Perceptron$$\n",
    "\n",
    "A form of ensemble learning, random forest regression is able to reduce the rmse, but it failed to capture the magnitude of the spikes in the test set, even when polynomial features are introduced. The averaging effect of the individual learner's result tends to push the prediction towards the center of the curve, thus minimizing rmse, but we lose the ability to accurately predict the last few transitions in the test set, which represents the California wildfire in 2020.\n",
    "\n",
    "Multiple linear regression did better than random forest regression because we could use alpha to control the level of regularization so that we don't overfit too much. The linear model is also good for inference. By extracting the coefficients, we found that the weekly AQI is primarily influenced by the past few week's week number, wind speed, temperature, and NO2 emissions. This result validated our research question. At the same time, however, we see that the linear model has tried its best on fitting the curve, but it could not accurately predict the last few spikes in the test set.\n",
    "\n",
    "Symbolic regression produces a result that looks kind of the same as multiple linear regression. However, symbolic regression takes a great amount of time to tune and train, and it is very bad for inference. We cannot possibly understand the result produced by a 200 level depth tree, but we can understand what features are important by checking the coefficients for the linear model. The good aspect of symbolic regression is that it could introduce more aspects of linearity, even periodicity (sine, cosine) into the model. That could potentially improve the fit.\n",
    "\n",
    "Multiple layer perceptron is the best performing model. As a neural network, it is capable of discovering a lot of hidden information and potential features that are beyond human interpretation. Even though it is bad for inference, we can use it to accurately predict future AQIs. This model is the only model that could predict the last few spikes in the test set with great accuracy on both location and magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-innocent",
   "metadata": {},
   "source": [
    "## Discussion of potential societal impacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-runner",
   "metadata": {},
   "source": [
    "It is important to have breathable fresh air for future generations, but with the ongoing development in power sectors and the increase in global greenhouse emissions, we are observing a decrease in air quality in some major cities around the globe. Hence, having an accurate AQI prediction model is crucial, as it allows government officials and regulatory bodies to plan for societal development such as controlling the emission of power plants, the amount of traffic on roads, and the use of electricity after-work hours. Additionally, an accurate model could also yield insights into how each factor influences the air quality index. This piece of information allows the concerned parties to enact measures to reach better air quality with a higher success rate. On the other hand, from an ethical standpoint, as humans, we should be responsible for the greenhouse emissions and the pollutions that we generate. The study on predicting AQI allows us to be more aware of how badly these factors influence the air we breathe and propels every one of us to make changes to keep the air healthy and fresh. In terms of data collection and analysis, we believe this project has no ethical issue. We are using public datasets from the U.S. government and the data entries do not contain sensitive personal information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-miniature",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-watts",
   "metadata": {},
   "source": [
    "Predicting the air quality accurately and efficiently can provide insights to environmental protection plans and alert the global community on regulating greenhouse emissions. In this project, we collected datasets containing precipitation, temperature, wind speed, pollutant emissions, and AQI to investigate this topic. From the EDA process, we found relationships between features such as temperature, wind speed, time_delta, and pollutants emission with respect to the AQI. We then proposed a pipeline to collect the appropriate data ranges from 2016-2020 in the city of LA. Knowing that the previous week's features could influence the current week's AQI, we apply a sliding window to the final dataset. Then, we construct four models including linear regression, symbolic regression, multiple layer perceptron, and random forest regression for the prediction task. We also devised a polynomial feature and the inclusion of the previous week's AQI as two ways to improve the prediction results. For each model, we conduct cross-validation to tune the parameters. The best performing model is then trained on the entire training set and generated predictions for the test set. Helpful graphics have been created to better visualize the prediction's trend as compared to the actual values. From the above sections of analysis, we found that random forest regression performed the best, thanks to ensemble learning. We also learned the influence of regularization on the model's performance. By tuning the regularization parameter, we can often find a sweet spot that sacrifices some fits on training but achieve a much better result on the test set. This result is applicable to linear regression and multiple layer perceptron. Lastly, from all four models, we see different levels of performance on the test set using the visualization from the previous section. We could see that our assumption to use time-series temperature, precipitation, wind speed, and pollutant emission to predict AQI is indeed doable. In particular, the best-tuned multiple-layer perceptron is able to predict pretty well as compared to other models on the test set. We are very happy to see this result, and now we have a functioning regressor that can be applied with even current data to predict the AQI for the future (at decent accuracy)! Very exciting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quantitative-uncle",
   "metadata": {},
   "source": [
    "## Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-finding",
   "metadata": {},
   "source": [
    "Using a sliding window approach, the current features and models could capture the regression problem on air quality to a certain extent, but not a hundred percent. During literature research, we found other features such as the level of ongoing traffic, air turbulence, special events (wildfire, riots), could also influence the air quality. However, we cannot find datasets with the same level of granularity as the datasets selected in this project, so unfortunately we did not cover these features. In the future, we could conduct field research and gather this information by ourselves, following the principles of data science.\n",
    "\n",
    "Meanwhile, the project is based on some levels of generalization: we take the average of every feature across all CBSA locations in California, which influences the accuracy of the model to a certain extent. Based on the EDA, we know the geographical location could influence air quality, but the effect is not significant, so we took the average. In the future, we could conduct a more fine-grained training process that takes account of the geographical location. Additionally, in this project, every row in the final dataframe is a weekly summary. We could try other granularity in terms of time, and see the effect of the sliding window.\n",
    "\n",
    "Lastly, RNN is a powerful model for recurrent information. Unfortunately, we don't have the time to implement such a model. We could try it in the future.\n",
    "\n",
    "As we inspected various regions while analyzing different datasets, we realized that in order to get a more accurate AQI prediction, the predicting region needs to be limit to a small region. That is, the AQI of the Los Angeles area is much easier to predict than that of California. \n",
    "\n",
    "As the area of interest gets narrowed down, the geological condition of the predicted area could become a crucial feature. For example, we observed that for every week between January 2016 and 2020, the average wind direction comes from only South and West. This suggests that the air quality of the region to the South and West side of LA may influence the AQI in LA much more than the other surrounding area. In the future, we will consider the effect of geographic location into account and use the information of air quality of the surrounding regions of our interesting area to help build a more accurate prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exotic-saying",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-baseline",
   "metadata": {},
   "source": [
    "[1] https://www.epa.gov/air-research/air-quality-and-climate-change-research\n",
    "\n",
    "[2] https://www.weather.gov/wrn/summer-article-clearing-the-air\n",
    "\n",
    "[3] https://www.mdpi.com/2076-3417/10/24/9151/pdf\n",
    "\n",
    "[4] https://www.hindawi.com/journals/complexity/2020/8049504/\n",
    "\n",
    "[5] https://towardsdatascience.com/hyperlocal-air-quality-prediction-using-machine-learning-ed3a661b9a71\n",
    "\n",
    "[6] https://towardsdatascience.com/ml-approaches-for-time-series-4d44722e48fe\n",
    "\n",
    "[7] https://towardsdatascience.com/walking-through-support-vector-regression-and-lstms-with-stock-price-prediction-45e11b620650\n",
    "\n",
    "[8] https://towardsdatascience.com/an-end-to-end-project-on-time-series-analysis-and-forecasting-with-python-4835e6bf050b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d6e775",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
